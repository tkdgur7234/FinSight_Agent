import feedparser
import requests
import os
import json
import re
from datetime import datetime
from time import mktime
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# =========================================================
# â–¼â–¼â–¼ [ì„¤ì •] ì¢…ëª© ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
TARGET_STOCKS = [
    {
        "ticker": "TSLA",         
        "name": "Tesla",
        "fetch_limit": 50,
        "avg_velocity": 10,
        "use_naver": False 
    },
    {
        "ticker": "005930",       
        "name": "ì‚¼ì„±ì „ì",
        "fetch_limit": 50,
        "avg_velocity": 20,
        "use_naver": True 
    }
    # êµ¬ê¸€(ì•ŒíŒŒë²³)ì€ ìš”ì²­ëŒ€ë¡œ ì œì™¸í•¨
]

MODEL_FAST = "solar-1-mini-chat"
MODEL_SMART = "solar-pro2"

SPAM_KEYWORDS = ["crypto", "whatsapp", "telegram", "giveaway", "free", "discord", "ë¦¬ë”©", "ë¬´ë£Œ", "ì¹´í†¡", "band"]

def clean_text(text):
    # HTML íƒœê·¸ ì œê±° ë° ê³µë°± ì •ë¦¬
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def parse_json_safely(text):
    try:
        text = text.strip()
        text = text.replace("```json", "").replace("```", "")
        
        start_idx = -1
        end_idx = -1

        if '[' in text and ']' in text:
            start_idx = text.find('[')
            end_idx = text.rfind(']') + 1
        elif '{' in text and '}' in text:
            start_idx = text.find('{')
            end_idx = text.rfind('}') + 1
            
        if start_idx != -1 and end_idx != -1:
            clean_json = text[start_idx:end_idx]
            return json.loads(clean_json)
        
        return json.loads(text)
    except Exception as e:
        return None

def check_volume_spike(posts, avg_velocity):
    if len(posts) < 5: return "ë°ì´í„° ë¶€ì¡±", 0
    try:
        newest_date = posts[0]['dt']
        oldest_date = posts[-1]['dt']
        diff_seconds = (newest_date - oldest_date).total_seconds()
        diff_hours = diff_seconds / 3600
        if diff_hours <= 0: diff_hours = 0.01
        velocity = len(posts) / diff_hours
        ratio = velocity / avg_velocity if avg_velocity > 0 else 1.0
        
        status = "Normal"
        if ratio > 2.5: status = "ğŸ”¥ Volume Spike"
        elif ratio > 1.5: status = "âš ï¸ Active"
        return status, round(velocity, 1)
    except:
        return "Calc Error", 0

def get_reddit_posts(ticker, limit):
    rss_url = f"https://www.reddit.com/r/stocks+wallstreetbets+investing+technology/search.rss?q={ticker}&sort=new&restrict_sr=on&limit=100"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    posts = []
    print(f"ğŸ” [Reddit] {ticker} ìˆ˜ì§‘ ì‹œë„ (Max 100)...")
    
    try:
        resp = requests.get(rss_url, headers=headers, timeout=10)
        
        if resp.status_code != 200:
            print(f"   -> Reddit ìš”ì²­ ì‹¤íŒ¨ (Code: {resp.status_code})")
            return []

        feed = feedparser.parse(resp.content)
        
        if not feed.entries:
            print("   -> Reddit ë°ì´í„° 0ê±´")
            return []

        for entry in feed.entries:
            if len(posts) >= limit: break
            
            content = clean_text(entry.description) if 'description' in entry else ""
            full_text = f"{entry.title} {content}"
            
            if len(full_text) < 10: continue
            if any(k in full_text.lower() for k in SPAM_KEYWORDS): continue
            
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime.fromtimestamp(mktime(entry.published_parsed))
            else:
                dt = datetime.now()
                
            posts.append({"text": full_text[:500], "dt": dt})
            
    except Exception as e:
        print(f"   -> Reddit Error: {e}")
        return []
        
    return posts

def get_naver_posts(code, limit):
    """
    ë„¤ì´ë²„ ê¸ˆìœµ PC ë²„ì „ HTML í¬ë¡¤ë§
    """
    posts = []
    
    if not code.isdigit():
        print(f"âš ï¸ [Naver] í•´ì™¸ì£¼ì‹({code})ì€ ì§€ì›í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
        return []

    print(f"ğŸ” [Naver HTML] {code} PC ì¢…í† ë°© ìˆ˜ì§‘ ì‹œë„...")
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    page = 1
    max_page = 5 
    
    while len(posts) < limit and page <= max_page:
        try:
            url = f"https://finance.naver.com/item/board.naver?code={code}&page={page}"
            res = requests.get(url, headers=headers, timeout=5)
            
            if res.status_code != 200:
                print(f"   -> í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {res.status_code}")
                break

            # [ì¸ì½”ë”© ìˆ˜ì •] í•œê¸€ ê¹¨ì§ ë°©ì§€ (euc-kr)
            try:
                html_text = res.content.decode('euc-kr', 'replace')
            except UnicodeDecodeError:
                html_text = res.content.decode('utf-8', 'replace')
                
            soup = BeautifulSoup(html_text, 'html.parser')
            rows = soup.select("div.section.inner_sub table.type2 tbody tr")
            
            if not rows:
                break

            for row in rows:
                if len(posts) >= limit: break
                
                title_tag = row.select_one("td.title a")
                if not title_tag: continue
                
                title = title_tag.get("title", "").strip()
                if not title:
                    title = title_tag.text.strip()
                
                # ë‚ ì§œ ì¶”ì¶œ
                date_tag = row.select_one("td:nth-of-type(6) span")
                date_str = date_tag.text.strip() if date_tag else ""
                
                try:
                    dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                except:
                    dt = datetime.now()

                full_text = clean_text(title)
                
                if len(full_text) < 2: continue
                if any(k in full_text for k in SPAM_KEYWORDS): continue
                
                posts.append({"text": full_text[:300], "dt": dt})
            
            page += 1
            
        except Exception as e:
            print(f"   -> Naver HTML Crawl Error: {e}")
            break
            
    return posts

def summarize_with_llm(ticker, posts):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    # [í•µì‹¬ ìˆ˜ì •] ì…ë ¥ í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ (ê³¼ë„í•œ í† í° ë°©ì§€)
    # 50ê°œ ê¸€ì„ ë‹¤ í•©ì¹˜ë©´ ë„ˆë¬´ ê¸¸ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìµœëŒ€ 3000ìê¹Œì§€ë§Œ ìë¦…ë‹ˆë‹¤.
    full_content = "\n".join([f"- {p['text']}" for p in posts])
    if len(full_content) > 3000:
        full_content = full_content[:3000] + "...(truncated)"
    
    # ë””ë²„ê¹…: ì…ë ¥ ê¸¸ì´ í™•ì¸
    # print(f"   -> LLM ì…ë ¥ ê¸¸ì´: {len(full_content)}ì")

    system_prompt = f"""
    Filter out noise from the comments about {ticker}.
    Select exactly **10 most meaningful sentences**.
    Output format must be a pure JSON list of strings: ["Msg 1", "Msg 2"]
    """

    try:
        # [ìˆ˜ì •] timeout ì„¤ì • ì¶”ê°€ (20ì´ˆ)
        response = client.chat.completions.create(
            model=MODEL_FAST,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": full_content}
            ],
            temperature=0.1,
            timeout=20 
        )
        content = response.choices[0].message.content
        result = parse_json_safely(content)
        return result if isinstance(result, list) else []
    except Exception as e:
        # [ìˆ˜ì •] ì—ëŸ¬ ìƒì„¸ ì¶œë ¥
        print(f"   -> âŒ ìš”ì•½ ì‹¤íŒ¨ (LLM Error): {str(e)}")
        return []

def analyze_final_sentiment(ticker, key_sentences):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(key_sentences)])
    
    system_prompt = f"""
    Analyze investor sentiment for {ticker}.
    Output JSON:
    {{
        "score": <int 0-100>,
        "status": "<Fear/Neutral/Greed>",
        "reason_korean": "<Explain in Korean>"
    }}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_SMART, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": sentences_text}
            ],
            temperature=0.1,
            timeout=20
        )
        content = response.choices[0].message.content
        return parse_json_safely(content)
    except Exception as e:
        print(f"   -> âŒ ë¶„ì„ ì‹¤íŒ¨ (LLM Error): {str(e)}")
        return None

def get_sentiment_analysis():
    results = []
    print("ğŸš€ ì»¤ë®¤ë‹ˆí‹° ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    for stock in TARGET_STOCKS:
        try:
            ticker = stock["ticker"]
            limit = stock["fetch_limit"]
            
            use_naver = stock.get("use_naver", False)
            
            # í•´ì™¸ì£¼ì‹ HTML í¬ë¡¤ë§ ë¶ˆê°€ -> ê°•ì œ Reddit
            if use_naver and not ticker.isdigit():
                print(f"âš ï¸ [{stock['name']}] ë„¤ì´ë²„ PC ê²Œì‹œíŒ ë¯¸ì§€ì› -> Reddit ì „í™˜")
                use_naver = False

            if use_naver:
                raw_posts = get_naver_posts(ticker, limit)
            else:
                raw_posts = get_reddit_posts(ticker, limit)
                
            if not raw_posts: 
                print(f"âš ï¸ [{stock['name']}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (0ê±´).")
                continue
            
            # 2. ë¶„ì„
            vol_status, velocity = check_volume_spike(raw_posts, stock["avg_velocity"])
            filtered_count = len(raw_posts)
            
            print(f"ğŸ¤– [{stock['name']}] ìš”ì•½ ì¤‘ ({filtered_count}ê±´)...")
            key_sentences = summarize_with_llm(stock["name"], raw_posts)
            
            if not key_sentences: 
                # [ìˆ˜ì •] ìš”ì•½ ì‹¤íŒ¨í•´ë„ ë¹ˆ ê»ë°ê¸°ëŠ” ë§Œë“¤ì§€ ì•Šê³  ìŠ¤í‚µ (ë¡œê·¸ëŠ” ìœ„ì—ì„œ ì¶œë ¥ë¨)
                continue
            
            print(f"ğŸ§  [{stock['name']}] ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
            final_data = analyze_final_sentiment(stock["name"], key_sentences)
            
            if final_data:
                final_data["ticker"] = stock["name"]
                final_data["volume_status"] = vol_status
                final_data["velocity"] = velocity
                final_data["filtered_count"] = filtered_count
                final_data["summary_sentences"] = key_sentences
                results.append(final_data)
                print(f"   -> âœ… ì™„ë£Œ: {stock['name']}")
                
        except Exception as e:
            print(f"âŒ [{stock.get('name')}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            continue
            
    return results