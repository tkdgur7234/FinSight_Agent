import feedparser
import requests
import os
import json
import re
from datetime import datetime
from time import mktime
from bs4 import BeautifulSoup
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# =========================================================
# â–¼â–¼â–¼ [ì„¤ì •] ì¢…ëª© ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
# ì´ì œ 'avg_velocity'ëŠ” ì´ˆê¸°ê°’ì¼ ë¿, ë°ì´í„°ê°€ ìŒ“ì´ë©´ ë¬´ì‹œë©ë‹ˆë‹¤.
TARGET_STOCKS = [
    {
        "ticker": "TSLA",         
        "name": "Tesla",
        "fetch_limit": 50,
        "avg_velocity": 10, # ì´ˆê¸°ê°’ (ë°ì´í„° ì—†ì„ ë•Œ ì‚¬ìš©)
        "use_naver": False 
    },
    {
        "ticker": "RKLB",         
        "name": "Rocket Lab",
        "fetch_limit": 50,
        "avg_velocity": 10, # ì´ˆê¸°ê°’ (ë°ì´í„° ì—†ì„ ë•Œ ì‚¬ìš©)
        "use_naver": False 
    },
    #{
    #   "ticker": "005930",       
    #    "name": "ì‚¼ì„±ì „ì",
    #    "fetch_limit": 50,
    #    "avg_velocity": 20, # ì´ˆê¸°ê°’
    #    "use_naver": True 
    #}
]

MODEL_FAST = "solar-1-mini-chat"
MODEL_SMART = "solar-pro2"
HISTORY_FILE = "velocity_history.json"  # ì†ë„ ê¸°ë¡ ì €ì¥ íŒŒì¼

SPAM_KEYWORDS = ["whatsapp", "telegram", "giveaway", "free", "discord", "ë¦¬ë”©", "ë¬´ë£Œ", "ì¹´í†¡", "ë°´ë“œ", "ê°€ì…", "ê³ ìˆ˜ìµ", "ì…ì¥"]

def clean_text(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def parse_json_safely(text):
    try:
        text = text.strip()
        text = text.replace("```json", "").replace("```", "")
        
        start_idx = -1
        end_idx = -1

        if '[' in text and ']' in text:
            start_idx = text.find('[')
            end_idx = text.rfind(']') + 1
        elif '{' in text and '}' in text:
            start_idx = text.find('{')
            end_idx = text.rfind('}') + 1
            
        if start_idx != -1 and end_idx != -1:
            clean_json = text[start_idx:end_idx]
            return json.loads(clean_json)
        
        return json.loads(text)
    except Exception as e:
        return None

# ---------------------------------------------------------
# [ì‹ ê·œ ê¸°ëŠ¥] íŒŒì¼ ê¸°ë°˜ ì†ë„ ë°ì´í„° ê´€ë¦¬
# ---------------------------------------------------------
def load_velocity_history():
    """ê¸°ë¡ëœ ì†ë„ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤."""
    if not os.path.exists(HISTORY_FILE):
        return {}
    try:
        with open(HISTORY_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except:
        return {}

def save_velocity_history(history):
    """ì†ë„ ë°ì´í„°ë¥¼ íŒŒì¼ì— ì €ì¥í•©ë‹ˆë‹¤."""
    try:
        with open(HISTORY_FILE, "w", encoding="utf-8") as f:
            json.dump(history, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"âš ï¸ History Save Error: {e}")

def get_dynamic_avg_velocity(ticker, default_val):
    """
    [í•µì‹¬] ì €ì¥ëœ ê¸°ë¡ì„ ë°”íƒ•ìœ¼ë¡œ 'ë™ì  í‰ê·  ì†ë„'ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    ìµœê·¼ 10ë²ˆì˜ ê¸°ë¡ í‰ê· ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    history = load_velocity_history()
    records = history.get(ticker, [])
    
    if not records:
        return default_val # ê¸°ë¡ ì—†ìœ¼ë©´ ì„¤ì •ê°’ ì‚¬ìš©
    
    # [ë³€ê²½] ë”•ì…”ë„ˆë¦¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ 'velocity' ê°’ë§Œ ì¶”ì¶œ
    # ì˜ˆ: [{'date': '...', 'velocity': 10}, ...] -> [10, 15, ...]
    velocities = []
    for r in records:
        if isinstance(r, dict) and 'velocity' in r:
            velocities.append(r['velocity'])
        elif isinstance(r, (int, float)): # í˜¸í™˜ì„±: ì˜›ë‚  ìˆ«ì ë°ì´í„°ê°€ ìˆë‹¤ë©´ í¬í•¨
            velocities.append(r)
            
    if not velocities:
        return default_val

    # ìµœê·¼ 14ì¼(2ì£¼) ì¹˜ í‰ê·  ì‚¬ìš©
    recent_velocities = velocities[-14:]
    avg = sum(recent_velocities) / len(recent_velocities)
    
    return avg

def update_velocity_history(ticker, current_velocity):
    """
    ì˜¤ëŠ˜ ë‚ ì§œì˜ ê¸°ë¡ì´ ì´ë¯¸ ìˆìœ¼ë©´ 'ê°±ì‹ (ë®ì–´ì“°ê¸°)'í•˜ê³ ,
    ì—†ìœ¼ë©´ 'ì¶”ê°€(Append)'í•©ë‹ˆë‹¤.
    """
    if current_velocity <= 0: return

    history = load_velocity_history()
    if ticker not in history:
        history[ticker] = []
    
    today_str = datetime.now().strftime("%Y-%m-%d")
    records = history[ticker]
    
    # [í•µì‹¬ ë¡œì§] ë§ˆì§€ë§‰ ê¸°ë¡ì´ ì˜¤ëŠ˜ì¸ì§€ í™•ì¸
    is_today_exist = False
    
    if records:
        last_record = records[-1]
        # ê¸°ë¡ì´ ë”•ì…”ë„ˆë¦¬ í˜•íƒœì´ê³ , ë‚ ì§œê°€ ì˜¤ëŠ˜ì´ë©´
        if isinstance(last_record, dict) and last_record.get('date') == today_str:
            # ì˜¤ëŠ˜ì˜ ê¸°ë¡ì„ ìµœì‹  ê°’ìœ¼ë¡œ ì—…ë°ì´íŠ¸ (ë®ì–´ì“°ê¸°)
            last_record['velocity'] = current_velocity
            is_today_exist = True
            
    # ì˜¤ëŠ˜ ê¸°ë¡ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ì¶”ê°€
    if not is_today_exist:
        records.append({
            "date": today_str,
            "velocity": current_velocity
        })
    
    # ìµœê·¼ 60ì¼ ë°ì´í„°ë§Œ ìœ ì§€
    if len(records) > 60:
        history[ticker] = records[-60:]
        
    save_velocity_history(history)

def check_volume_spike(ticker, posts, default_velocity):
    if len(posts) < 5: return "ë°ì´í„° ë¶€ì¡±", 0
    try:
        newest_date = posts[0]['dt']
        oldest_date = posts[-1]['dt']
        diff_seconds = (newest_date - oldest_date).total_seconds()
        diff_hours = diff_seconds / 3600
        if diff_hours <= 0: diff_hours = 0.01
        
        # 1. í˜„ì¬ ì†ë„ ê³„ì‚°
        current_velocity = len(posts) / diff_hours
        
        # 2. [ë³€ê²½] ë™ì  í‰ê·  ì†ë„ ê°€ì ¸ì˜¤ê¸° (DB ëŒ€ìš©)
        # ê¸°ë¡ëœ í‰ê· ì„ ìš°ì„  ì‚¬ìš©í•˜ê³ , ì—†ìœ¼ë©´ default_velocity ì‚¬ìš©
        avg_velocity = get_dynamic_avg_velocity(ticker, default_velocity)
        
        # 3. ì´ë²ˆ ì¸¡ì •ê°’ì„ ê¸°ë¡ì— ì €ì¥ (ë‹¤ìŒë²ˆ í‰ê· ì„ ìœ„í•´)
        # ë‹¨, 'ë°ì´í„° ë¶€ì¡±'ì´ê±°ë‚˜ ì´ìƒì¹˜ì¼ ë•ŒëŠ” ì €ì¥ ì•ˆ í•  ìˆ˜ë„ ìˆìŒ
        update_velocity_history(ticker, current_velocity)

        ratio = current_velocity / avg_velocity if avg_velocity > 0 else 1.0
        
        status = "Normal"
        if ratio > 2.5: status = "ğŸ”¥ Volume Spike"
        elif ratio > 1.5: status = "âš ï¸ Active"
        
        # ë””ë²„ê¹…ìš© ë¡œê·¸
        print(f"   -> â±ï¸ ì†ë„: {current_velocity:.1f} (í‰ê· : {avg_velocity:.1f}) | ë¹„ìœ¨: {ratio:.1f}ë°°")
        
        return status, round(current_velocity, 1)
    except Exception as e:
        print(f"Calc Error: {e}")
        return "Calc Error", 0

def get_reddit_posts(ticker, limit):
    rss_url = f"https://www.reddit.com/r/stocks+wallstreetbets+investing+technology/search.rss?q={ticker}&sort=new&restrict_sr=on&limit=100"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    posts = []
    print(f"ğŸ” [Reddit] {ticker} ìˆ˜ì§‘ ì‹œë„ (Max 100)...")
    
    try:
        resp = requests.get(rss_url, headers=headers, timeout=10)
        if resp.status_code != 200:
            return []

        feed = feedparser.parse(resp.content)
        if not feed.entries:
            return []

        for entry in feed.entries:
            if len(posts) >= limit: break
            content = clean_text(entry.description) if 'description' in entry else ""
            full_text = f"{entry.title} {content}"
            
            if len(full_text) < 10: continue
            if any(k in full_text.lower() for k in SPAM_KEYWORDS): continue
            
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime.fromtimestamp(mktime(entry.published_parsed))
            else:
                dt = datetime.now()
            posts.append({"text": full_text[:500], "dt": dt})
    except:
        return []
    return posts

def get_naver_posts(code, limit):
    posts = []
    if not code.isdigit(): return []

    print(f"ğŸ” [Naver HTML] {code} PC ì¢…í† ë°© ìˆ˜ì§‘ ì‹œë„...")
    headers = {'User-Agent': 'Mozilla/5.0'}
    page = 1
    
    while len(posts) < limit and page <= 5:
        try:
            url = f"https://finance.naver.com/item/board.naver?code={code}&page={page}"
            res = requests.get(url, headers=headers, timeout=5)
            if res.status_code != 200: break

            try:
                html_text = res.content.decode('utf-8')
            except:
                html_text = res.content.decode('cp949', 'ignore')
                
            soup = BeautifulSoup(html_text, 'html.parser')
            rows = soup.select("div.section.inner_sub table.type2 tbody tr")
            if not rows: break

            for row in rows:
                if len(posts) >= limit: break
                title_tag = row.select_one("td.title a")
                if not title_tag: continue
                
                title = title_tag.get("title", "").strip() or title_tag.text.strip()
                date_tag = row.select_one("td:nth-of-type(6) span")
                date_str = date_tag.text.strip() if date_tag else ""
                
                try:
                    dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                except:
                    dt = datetime.now()

                full_text = clean_text(title)
                if len(full_text) < 2: continue
                if any(k in full_text for k in SPAM_KEYWORDS): continue
                posts.append({"text": full_text[:300], "dt": dt})
            page += 1
        except:
            break
    return posts

def summarize_with_llm(ticker, posts):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    full_content = "\n".join([f"- {p['text']}" for p in posts])
    if len(full_content) > 3000:
        full_content = full_content[:3000] + "...(truncated)"

    system_prompt = f"""
    Filter out noise from the comments about {ticker}.
    Select exactly **10 most meaningful sentences/titles**.
    Output format must be a pure JSON list: ["Opinion 1", "Opinion 2"]
    """
    try:
        response = client.chat.completions.create(
            model=MODEL_FAST,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": full_content}],
            temperature=0.1, timeout=30
        )
        return parse_json_safely(response.choices[0].message.content) or []
    except:
        return []

def analyze_final_sentiment(ticker, key_sentences):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(key_sentences)])
    system_prompt = f"""
    Analyze investor sentiment for {ticker}.
    Output JSON: {{ "score": <0-100>, "status": "<Extreme Fear/Fear/Neutral/Greed/Extreme Greed>", "reason_korean": "..." }}
    """
    try:
        response = client.chat.completions.create(
            model=MODEL_SMART,
            messages=[{"role": "system", "content": system_prompt}, {"role": "user", "content": sentences_text}],
            temperature=0.1, timeout=30
        )
        return parse_json_safely(response.choices[0].message.content)
    except:
        return None

def get_sentiment_analysis():
    results = []
    print("ğŸš€ ì»¤ë®¤ë‹ˆí‹° ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    for stock in TARGET_STOCKS:
        try:
            ticker = stock["ticker"]
            limit = stock["fetch_limit"]
            
            if ticker.isdigit():
                raw_posts = get_naver_posts(ticker, limit)
            else:
                raw_posts = get_reddit_posts(ticker, limit)
                
            if not raw_posts: 
                print(f"âš ï¸ [{stock['name']}] ë°ì´í„° ì—†ìŒ (0ê±´).")
                continue
            
            # [ìˆ˜ì •] check_volume_spikeì— tickerë¥¼ ì „ë‹¬í•˜ì—¬ íˆìŠ¤í† ë¦¬ ê´€ë¦¬
            vol_status, velocity = check_volume_spike(stock["name"], raw_posts, stock["avg_velocity"])
            filtered_count = len(raw_posts)
            
            print(f"ğŸ¤– [{stock['name']}] ìš”ì•½ ì¤‘ ({filtered_count}ê±´)...")
            key_sentences = summarize_with_llm(stock["name"], raw_posts)
            if not key_sentences: continue
            
            print(f"ğŸ§  [{stock['name']}] ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
            final_data = analyze_final_sentiment(stock["name"], key_sentences)
            
            if final_data:
                final_data["ticker"] = stock["name"]
                final_data["volume_status"] = vol_status
                final_data["velocity"] = velocity
                final_data["filtered_count"] = filtered_count
                final_data["summary_sentences"] = key_sentences
                results.append(final_data)
                print(f"   -> âœ… ì™„ë£Œ: {stock['name']}")
                
        except Exception as e:
            print(f"âŒ [{stock.get('name')}] ì˜¤ë¥˜: {e}")
            continue
            
    return results