import feedparser
import requests
import os
import json
import re
from datetime import datetime
from time import mktime
from bs4 import BeautifulSoup  # HTML íŒŒì‹±ì„ ìœ„í•´ ì¶”ê°€
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# =========================================================
# â–¼â–¼â–¼ [ì„¤ì •] ì¢…ëª© ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
TARGET_STOCKS = [
    {
        "ticker": "TSLA",         
        "name": "Tesla",
        "fetch_limit": 50,
        "avg_velocity": 10,
        "use_naver": False   # í•´ì™¸ì£¼ì‹ -> Reddit ê¶Œì¥
    },
    {
        "ticker": "005930",       
        "name": "ì‚¼ì„±ì „ì",
        "fetch_limit": 50,
        "avg_velocity": 20,
        "use_naver": True    # êµ­ë‚´ì£¼ì‹ -> Naver HTML í¬ë¡¤ë§
    },
    {
        "ticker": "GOOG.O",       
        "name": "ì•ŒíŒŒë²³(êµ¬ê¸€)",
        "fetch_limit": 30,    
        "avg_velocity": 5,
        "use_naver": False   # í•´ì™¸ì£¼ì‹ì€ ë„¤ì´ë²„ HTML ê²Œì‹œíŒì´ ì—†ìœ¼ë¯€ë¡œ Falseë¡œ ì„¤ì •
    }
]

MODEL_FAST = "solar-1-mini-chat"
MODEL_SMART = "solar-pro2"

SPAM_KEYWORDS = ["crypto", "whatsapp", "telegram", "giveaway", "free", "discord", "ë¦¬ë”©", "ë¬´ë£Œ", "ì¹´í†¡", "band"]

def clean_text(text):
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def parse_json_safely(text):
    try:
        text = text.strip()
        text = text.replace("```json", "").replace("```", "")
        
        start_idx = -1
        end_idx = -1

        if '[' in text and ']' in text:
            start_idx = text.find('[')
            end_idx = text.rfind(']') + 1
        elif '{' in text and '}' in text:
            start_idx = text.find('{')
            end_idx = text.rfind('}') + 1
            
        if start_idx != -1 and end_idx != -1:
            clean_json = text[start_idx:end_idx]
            return json.loads(clean_json)
        
        return json.loads(text)
    except Exception as e:
        return None

def check_volume_spike(posts, avg_velocity):
    if len(posts) < 5: return "ë°ì´í„° ë¶€ì¡±", 0
    try:
        newest_date = posts[0]['dt']
        oldest_date = posts[-1]['dt']
        diff_seconds = (newest_date - oldest_date).total_seconds()
        diff_hours = diff_seconds / 3600
        if diff_hours <= 0: diff_hours = 0.01
        velocity = len(posts) / diff_hours
        ratio = velocity / avg_velocity if avg_velocity > 0 else 1.0
        
        status = "Normal"
        if ratio > 2.5: status = "ğŸ”¥ Volume Spike"
        elif ratio > 1.5: status = "âš ï¸ Active"
        return status, round(velocity, 1)
    except:
        return "Calc Error", 0

def get_reddit_posts(ticker, limit):
    rss_url = f"https://www.reddit.com/r/stocks+wallstreetbets+investing+technology/search.rss?q={ticker}&sort=new&restrict_sr=on&limit=100"
    
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    posts = []
    print(f"ğŸ” [Reddit] {ticker} ìˆ˜ì§‘ ì‹œë„ (Max 100)...")
    
    try:
        resp = requests.get(rss_url, headers=headers, timeout=10)
        
        if resp.status_code != 200:
            print(f"   -> Reddit ìš”ì²­ ì‹¤íŒ¨ (Code: {resp.status_code})")
            return []

        feed = feedparser.parse(resp.content)
        
        if not feed.entries:
            print("   -> Reddit ë°ì´í„° 0ê±´")
            return []

        for entry in feed.entries:
            if len(posts) >= limit: break
            
            content = clean_text(entry.description) if 'description' in entry else ""
            full_text = f"{entry.title} {content}"
            
            if len(full_text) < 10: continue
            if any(k in full_text.lower() for k in SPAM_KEYWORDS): continue
            
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                dt = datetime.fromtimestamp(mktime(entry.published_parsed))
            else:
                dt = datetime.now()
                
            posts.append({"text": full_text[:500], "dt": dt})
            
    except Exception as e:
        print(f"   -> Reddit Error: {e}")
        return []
        
    return posts

def get_naver_posts(code, limit):
    """
    [ì™„ì „ ë³€ê²½] ë„¤ì´ë²„ ê¸ˆìœµ PC ë²„ì „ HTML í¬ë¡¤ë§ (API ë¯¸ì‚¬ìš©)
    ëŒ€ìƒ URL: https://finance.naver.com/item/board.naver?code={code}
    """
    posts = []
    
    # 1. í•´ì™¸ ì£¼ì‹ ì²´í¬ (ìˆ«ìê°€ ì•„ë‹ˆë©´ ì§€ì› ë¶ˆê°€)
    if not code.isdigit():
        print(f"âš ï¸ [Naver] í•´ì™¸ì£¼ì‹({code})ì€ PC HTML ê²Œì‹œíŒì´ ì—†ì–´ í¬ë¡¤ë§ ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. (Reddit ì‚¬ìš© ê¶Œì¥)")
        return []

    print(f"ğŸ” [Naver HTML] {code} PC ì¢…í† ë°© ìˆ˜ì§‘ ì‹œë„...")
    
    # PC ë¸Œë¼ìš°ì € User-Agent
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
    }

    page = 1
    max_page = 5 # ë„ˆë¬´ ë§ì´ ê¸ì§€ ì•Šë„ë¡ ì œí•œ
    
    while len(posts) < limit and page <= max_page:
        try:
            url = f"https://finance.naver.com/item/board.naver?code={code}&page={page}"
            res = requests.get(url, headers=headers, timeout=5)
            
            if res.status_code != 200:
                print(f"   -> í˜ì´ì§€ ì ‘ì† ì‹¤íŒ¨: {res.status_code}")
                break

            # ì¸ì½”ë”© ì„¤ì • (í•œê¸€ ê¹¨ì§ ë°©ì§€)
            res.encoding = 'cp949' 
            soup = BeautifulSoup(res.text, 'html.parser')
            
            # í…Œì´ë¸” í–‰ ê°€ì ¸ì˜¤ê¸°
            rows = soup.select("div.section.inner_sub table.type2 tbody tr")
            
            if not rows:
                break

            for row in rows:
                if len(posts) >= limit: break
                
                # ë§ˆìš°ìŠ¤ ì˜¤ë²„ ì‹œ ë‚˜ì˜¤ëŠ” ë³¸ë¬¸ ë¯¸ë¦¬ë³´ê¸° or ì œëª©
                title_tag = row.select_one("td.title a")
                if not title_tag: continue
                
                title = title_tag.get("title", "").strip()
                if not title:
                    title = title_tag.text.strip()
                
                # ë‚ ì§œ ì¶”ì¶œ (YYYY.MM.DD HH:mm)
                date_tag = row.select_one("td:nth-of-type(6) span")
                date_str = date_tag.text.strip() if date_tag else ""
                
                try:
                    dt = datetime.strptime(date_str, "%Y.%m.%d %H:%M")
                except:
                    dt = datetime.now()

                full_text = clean_text(title)
                
                if len(full_text) < 2: continue
                if any(k in full_text for k in SPAM_KEYWORDS): continue
                
                posts.append({"text": full_text[:300], "dt": dt})
            
            page += 1
            
        except Exception as e:
            print(f"   -> Naver HTML Crawl Error: {e}")
            break
            
    return posts

def summarize_with_llm(ticker, posts):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    context_text = "\n".join([f"- {p['text']}" for p in posts])

    system_prompt = f"""
    Filter out noise from the comments about {ticker}.
    Select exactly **10 most meaningful sentences**.
    Output format must be a pure JSON list of strings: ["Msg 1", "Msg 2"]
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_FAST,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": context_text}
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content
        result = parse_json_safely(content)
        return result if isinstance(result, list) else []
    except Exception as e:
        print(f"Summary Error: {e}")
        return []

def analyze_final_sentiment(ticker, key_sentences):
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(key_sentences)])
    
    system_prompt = f"""
    Analyze investor sentiment for {ticker}.
    Output JSON:
    {{
        "score": <int 0-100>,
        "status": "<Fear/Neutral/Greed>",
        "reason_korean": "<Explain in Korean>"
    }}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_SMART, 
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": sentences_text}
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content
        return parse_json_safely(content)
    except Exception as e:
        print(f"Analysis Error: {e}")
        return None

def get_sentiment_analysis():
    results = []
    print("ğŸš€ ì»¤ë®¤ë‹ˆí‹° ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    for stock in TARGET_STOCKS:
        try:
            ticker = stock["ticker"]
            limit = stock["fetch_limit"]
            
            # [ìˆ˜ì •] ì‚¬ìš©ìê°€ ê°•ì œë¡œ use_naver=Trueë¥¼ í•´ë„, í•´ì™¸ì£¼ì‹(ë¬¸ì í‹°ì»¤)ì€ HTML í¬ë¡¤ë§ ë¶ˆê°€í•˜ë¯€ë¡œ ê°•ì œ Reddit ì „í™˜
            use_naver = stock.get("use_naver", False)
            
            if use_naver and not ticker.isdigit():
                print(f"âš ï¸ [{stock['name']}] ë„¤ì´ë²„ HTML í¬ë¡¤ë§ì€ êµ­ë‚´ì£¼ì‹(ìˆ«ìì½”ë“œ)ë§Œ ì§€ì›í•©ë‹ˆë‹¤. Redditìœ¼ë¡œ ì „í™˜í•©ë‹ˆë‹¤.")
                use_naver = False

            if use_naver:
                raw_posts = get_naver_posts(ticker, limit)
            else:
                raw_posts = get_reddit_posts(ticker, limit)
                
            if not raw_posts: 
                print(f"âš ï¸ [{stock['name']}] ìˆ˜ì§‘ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (0ê±´).")
                continue
            
            # 2. ë¶„ì„
            vol_status, velocity = check_volume_spike(raw_posts, stock["avg_velocity"])
            filtered_count = len(raw_posts)
            
            print(f"ğŸ¤– [{stock['name']}] ìš”ì•½ ì¤‘ ({filtered_count}ê±´)...")
            key_sentences = summarize_with_llm(stock["name"], raw_posts)
            
            if not key_sentences: 
                print(f"   -> ìš”ì•½ ì‹¤íŒ¨ (LLM ì‘ë‹µ ì˜¤ë¥˜)")
                continue
            
            print(f"ğŸ§  [{stock['name']}] ì‹¬ì¸µ ë¶„ì„ ì¤‘...")
            final_data = analyze_final_sentiment(stock["name"], key_sentences)
            
            if final_data:
                final_data["ticker"] = stock["name"]
                final_data["volume_status"] = vol_status
                final_data["velocity"] = velocity
                final_data["filtered_count"] = filtered_count
                final_data["summary_sentences"] = key_sentences
                results.append(final_data)
                print(f"   -> âœ… ì™„ë£Œ: {stock['name']}")
                
        except Exception as e:
            print(f"âŒ [{stock.get('name')}] ì²˜ë¦¬ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
            continue
            
    return results