import feedparser
import requests
import os
import json
import re
from datetime import datetime
from time import mktime
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# =========================================================
# â–¼â–¼â–¼ [ì‚¬ìš©ì ì„¤ì •] ì¢…ëª© ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
TARGET_STOCKS = [
    {
        "ticker": "TSLA",         # [ë¯¸êµ­] Reddit ê²€ìƒ‰ì–´
        "name": "Tesla",
        "fetch_limit": 50,
        "avg_velocity": 10
    },
    {
        "ticker": "005930",       # [í•œêµ­] ì¢…ëª©ì½”ë“œ (ì‚¼ì„±ì „ì)
        "name": "ì‚¼ì„±ì „ì",
        "fetch_limit": 50,
        "avg_velocity": 20
    }
]

# â–¼â–¼â–¼ [ëª¨ë¸ ì„¤ì •] Update: solar-pro -> solar-pro2 â–¼â–¼â–¼
MODEL_FAST = "solar-1-mini-chat"   # ë‹¨ìˆœ ìš”ì•½ìš©
MODEL_SMART = "solar-pro2"          # ê³ ì„±ëŠ¥ ë¶„ì„ìš©

SPAM_KEYWORDS = ["crypto", "whatsapp", "telegram", "giveaway", "free", "discord", "ë¦¬ë”©", "ë¬´ë£Œ", "ì¹´í†¡", "band"]

def clean_text(text):
    """íŠ¹ìˆ˜ë¬¸ì ë° ë¶ˆí•„ìš”í•œ ê³µë°± ì œê±°"""
    text = re.sub(r'<[^>]+>', '', text) # HTML íƒœê·¸ ì œê±°
    text = re.sub(r'\s+', ' ', text).strip()
    return text

def parse_json_safely(text):
    """
    [í•µì‹¬ ìˆ˜ì •] LLM ì‘ë‹µì—ì„œ JSON ë¶€ë¶„ë§Œ ì •ê·œì‹ìœ¼ë¡œ ì •ë°€ ì¶”ì¶œ
    ì˜¤ë¥˜ ì›ì¸: JSON ë’¤ì— ì¡ë‹´ì´ ì„ì—¬ ìˆìœ¼ë©´ json.loads()ê°€ í„°ì§
    """
    try:
        # 1. ```json ... ``` ì½”ë“œ ë¸”ë¡ ì œê±°
        text = text.replace("```json", "").replace("```", "").strip()
        
        # 2. ê°€ì¥ ê²‰ì— ìˆëŠ” {} ë˜ëŠ” [] ì°¾ê¸°
        # DOTALL: ì¤„ë°”ê¿ˆì´ ìˆì–´ë„ ë§¤ì¹­
        match = re.search(r'(\{.*\}|\[.*\])', text, re.DOTALL)
        if match:
            clean_json = match.group(1)
            return json.loads(clean_json)
        else:
            # ë§¤ì¹­ ì•ˆë˜ë©´ ì›ë³¸ ì‹œë„
            return json.loads(text)
    except Exception:
        return None

def check_volume_spike(posts, avg_velocity):
    """
    ê²Œì‹œê¸€ ë¦¬ì   ì†ë„ ê³„ì‚° (Naver/Reddit í†µí•© ì§€ì›)
    """
    if len(posts) < 5: return "ë°ì´í„° ë¶€ì¡±", 0

    try:
        # ìµœì‹  ê¸€ê³¼ ê°€ì¥ ì˜¤ë˜ëœ ê¸€ì˜ ì‹œê°„ ì°¨ì´ ê³„ì‚°
        newest_date = posts[0]['dt']
        oldest_date = posts[-1]['dt']
        
        # ì‹œê°„ ì°¨ì´ (ì‹œê°„ ë‹¨ìœ„)
        diff_seconds = (newest_date - oldest_date).total_seconds()
        diff_hours = diff_seconds / 3600
        
        if diff_hours <= 0: diff_hours = 0.01 # 0ìœ¼ë¡œ ë‚˜ëˆ„ê¸° ë°©ì§€

        velocity = len(posts) / diff_hours
        ratio = velocity / avg_velocity if avg_velocity > 0 else 1.0

        status = "Normal"
        if ratio > 2.5: status = "ğŸ”¥ Volume Spike"
        elif ratio > 1.5: status = "âš ï¸ Active"
        
        return status, round(velocity, 1)

    except Exception as e:
        # print(f"Velocity Calc Error: {e}")
        return "Calc Error", 0

def get_reddit_posts(ticker, limit):
    """Reddit RSS í¬ë¡¤ë§"""
    rss_url = f"https://www.reddit.com/r/stocks+wallstreetbets+investing+technology/search.rss?q={ticker}&sort=new&restrict_sr=on&limit={limit+20}"
    feed = feedparser.parse(rss_url)
    posts = []
    
    print(f"ğŸ” [Reddit] {ticker} ìˆ˜ì§‘ ì¤‘...")
    for entry in feed.entries:
        if len(posts) >= limit: break
        
        content = clean_text(entry.description) if 'description' in entry else ""
        full_text = f"{entry.title} {content}"
        
        if len(full_text) < 10: continue
        if any(k in full_text.lower() for k in SPAM_KEYWORDS): continue
        
        # ë‚ ì§œ í‘œì¤€í™” (struct_time -> datetime)
        dt = datetime.fromtimestamp(mktime(entry.published_parsed))
        
        posts.append({
            "text": full_text[:500],
            "dt": dt
        })
    return posts

def get_naver_posts(code, limit):
    """
    [ì—…ê·¸ë ˆì´ë“œ] ë„¤ì´ë²„ ëª¨ë°”ì¼ ì¦ê¶Œ API ì‚¬ìš© (JSON íŒŒì‹±)
    HTML íŒŒì‹±ë³´ë‹¤ ë¹ ë¥´ê³  ë‚ ì§œ ì •ë³´ë¥¼ ì •í™•íˆ ì–»ì„ ìˆ˜ ìˆìŒ
    """
    posts = []
    print(f"ğŸ” [Naver API] {code} ì¢…í† ë°© ìˆ˜ì§‘ ì¤‘...")
    
    # ë„¤ì´ë²„ ëª¨ë°”ì¼ ì¢…ëª©í† ë¡ ì‹¤ API URL
    url = f"https://m.stock.naver.com/api/discuss/local/{code}?offset=0&limit={limit+10}"
    headers = {
        'User-Agent': 'Mozilla/5.0 (iPhone; CPU iPhone OS 13_2_3 like Mac OS X)',
        'Referer': f'https://m.stock.naver.com/domestic/stock/{code}/discuss'
    }
    
    try:
        res = requests.get(url, headers=headers)
        data = res.json()
        
        # API êµ¬ì¡°: data --> ë¦¬ìŠ¤íŠ¸ í˜•íƒœ
        for item in data:
            if len(posts) >= limit: break
            
            title = item.get('title', '')
            contents = item.get('contents', '')
            full_text = f"{title} {contents}"
            full_text = clean_text(full_text)
            
            # ë‚ ì§œ íŒŒì‹± (APIëŠ” '2025-01-05 14:30:00' í˜•íƒœë¡œ ì¤Œ)
            date_str = item.get('date', '') # YYYY-MM-DD HH:MM:SS
            try:
                dt = datetime.strptime(date_str, "%Y-%m-%d %H:%M:%S")
            except:
                dt = datetime.now() # ì—ëŸ¬ ì‹œ í˜„ì¬ ì‹œê°„

            if len(full_text) < 5: continue
            if any(k in full_text for k in SPAM_KEYWORDS): continue
            
            posts.append({
                "text": full_text[:300], # ë„ˆë¬´ ê¸¸ë©´ ìë¦„
                "dt": dt
            })
            
    except Exception as e:
        print(f"Naver API Error: {e}")
        
    return posts

def summarize_with_llm(ticker, posts):
    """
    [2ì°¨ í•„í„°ë§ & ìš”ì•½] -> solar-1-mini-chat
    """
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    # ìµœê·¼ ê¸€ ìˆœì„œëŒ€ë¡œ í…ìŠ¤íŠ¸ ë³‘í•©
    context_text = "\n".join([f"- {p['text']}" for p in posts])

    system_prompt = f"""
    You are a data filtering assistant.
    Filter out noise from the comments about {ticker}.
    Select exactly **10 most meaningful sentences** that explain the current investor sentiment.
    
    Output format:
    A pure JSON list of strings. 
    Example: ["High expectation for earnings...", "Worried about CEO risk..."]
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_FAST,
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": context_text}
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content
        
        # [ìˆ˜ì •] ì•ˆì „í•œ JSON íŒŒì‹± í•¨ìˆ˜ ì‚¬ìš©
        parsed_data = parse_json_safely(content)
        if isinstance(parsed_data, list):
            return parsed_data
        else:
            return []
            
    except Exception as e:
        print(f"Summary Error: {e}")
        return []

def analyze_final_sentiment(ticker, key_sentences):
    """
    [ìµœì¢… ë¶„ì„] -> solar-pro2
    """
    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    sentences_text = "\n".join([f"{i+1}. {s}" for i, s in enumerate(key_sentences)])

    system_prompt = f"""
    You are an expert Stock Sentiment Analyst.
    Based on the key user opinions for {ticker}, provide a deep analysis.

    Output JSON Format:
    {{
        "score": <int 0-100>,
        "status": "<Fear/Neutral/Greed>",
        "reason_korean": "<Explain the reason in Korean>"
    }}
    """

    try:
        response = client.chat.completions.create(
            model=MODEL_SMART, # solar-pro2 ì‚¬ìš©
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": sentences_text}
            ],
            temperature=0.1
        )
        content = response.choices[0].message.content
        
        # [ìˆ˜ì •] ì•ˆì „í•œ JSON íŒŒì‹± í•¨ìˆ˜ ì‚¬ìš©
        return parse_json_safely(content)
        
    except Exception as e:
        print(f"Analysis Error: {e}")
        return None

def get_sentiment_analysis():
    results = []
    print("ğŸš€ ì»¤ë®¤ë‹ˆí‹° ê°ì„± ë¶„ì„ ì‹œì‘...")
    
    for stock in TARGET_STOCKS:
        ticker = stock["ticker"]
        limit = stock["fetch_limit"]
        
        # 1. ì†ŒìŠ¤ ë¶„ê¸° (ìˆ«ìë©´ ë„¤ì´ë²„, ì•„ë‹ˆë©´ Reddit)
        if ticker.isdigit():
            raw_posts = get_naver_posts(ticker, limit)
        else:
            raw_posts = get_reddit_posts(ticker, limit)
            
        if not raw_posts: 
            print(f"âš ï¸ {stock['name']} ë°ì´í„° ì—†ìŒ")
            continue
        
        # 2. Volume Spike (ì´ì œ ë„¤ì´ë²„ë„ ê°€ëŠ¥!)
        vol_status, velocity = check_volume_spike(raw_posts, stock["avg_velocity"])
        
        # 3. ìš”ì•½ (Mini)
        print(f"ğŸ¤– [{stock['name']}] í•µì‹¬ ìš”ì•½ ì¶”ì¶œ ì¤‘ ({MODEL_FAST})...")
        key_sentences = summarize_with_llm(stock["name"], raw_posts)
        
        if not key_sentences: 
            print("   -> ìš”ì•½ ì‹¤íŒ¨")
            continue
        
        # 4. ìµœì¢… ë¶„ì„ (Pro2)
        print(f"ğŸ§  [{stock['name']}] ê°ì„± ë¶„ì„ ì¤‘ ({MODEL_SMART})...")
        final_data = analyze_final_sentiment(stock["name"], key_sentences)
        
        if final_data:
            final_data["ticker"] = stock["name"]
            final_data["volume_status"] = vol_status
            final_data["velocity"] = velocity
            final_data["summary_sentences"] = key_sentences
            results.append(final_data)
            print("   -> ë¶„ì„ ì™„ë£Œ âœ…")
            
    return results