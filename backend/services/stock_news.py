import feedparser
import re
import os
import json
from html import unescape
from datetime import datetime, timedelta
from dateutil import parser as date_parser
from difflib import SequenceMatcher
import pytz
from openai import OpenAI
from dotenv import load_dotenv

load_dotenv()

# =========================================================
# â–¼â–¼â–¼ [ì„¤ì •] ê´€ì‹¬ ì¢…ëª© ë‰´ìŠ¤ ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
TARGET_STOCKS = [
    {
        "ticker": "TSLA",
        "name": "Tesla",
        "lang": "en",
        "limit": 2
    },
    {
        "ticker": "GOOG",
        "name": "Google",
        "lang": "en",
        "limit": 2
    },
    #{
    #    "ticker": "005930",
    #    "name": "ì‚¼ì„±ì „ì",
    #    "lang": "ko",
    #    "limit": 2
    #}
]

# =========================================================
# â–¼â–¼â–¼ [ì„¤ì •] ìœ ë£Œ(Paywall) ë‰´ìŠ¤ ì†ŒìŠ¤ ë¸”ë™ë¦¬ìŠ¤íŠ¸ â–¼â–¼â–¼
# =========================================================
PAYWALLED_SOURCES = [
    "Bloomberg",
    "The Wall Street Journal",
    "Financial Times",
    "Barron's",
    "The Information",
    "Seeking Alpha",
    "The Economist",
    "Business Insider",
    "MarketWatch",
    "Hankyung", 
    "Maeil Business Newspaper"
]

def clean_html(raw_html):
    cleanr = re.compile('<.*?>')
    cleantext = re.sub(cleanr, '', raw_html)
    return unescape(cleantext).strip()

def is_similar(a, b, threshold=0.5):
    """ì œëª© ìœ ì‚¬ë„ ê²€ì‚¬"""
    return SequenceMatcher(None, a, b).ratio() > threshold

def is_paywalled(source_name):
    if not source_name: return False
    source_lower = source_name.lower()
    for blocked in PAYWALLED_SOURCES:
        if blocked.lower() in source_lower:
            return True
    return False

def get_google_news_rss(query, lang="en", limit=2):
    """
    êµ¬ê¸€ ë‰´ìŠ¤ RSS í¬ë¡¤ë§ (ìƒì„¸ ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€)
    """
    
    # í†µê³„ ì§‘ê³„ìš© ë³€ìˆ˜
    stats = {
        "total_fetched": 0,
        "dropped_paywall": 0,
        "dropped_time": 0,
        "dropped_dup": 0,
        "accepted": 0
    }
    
    # 1. ë„‰ë„‰í•˜ê²Œ ê°€ì ¸ì˜¤ê¸°
    fetch_count = 15
    
    if lang == 'ko':
        rss_url = f"https://news.google.com/rss/search?q={query}&hl=ko&gl=KR&ceid=KR:ko"
    else:
        rss_url = f"https://news.google.com/rss/search?q={query}+when:24h&hl=en-US&gl=US&ceid=US:en"

    try:
        feed = feedparser.parse(rss_url)
        news_results = []
        seen_titles = [] 
        
        # ì‹œê°„ í•„í„° ì„¤ì •
        kst_tz = pytz.timezone('Asia/Seoul')
        now_kst = datetime.now(kst_tz)
        cutoff_end = now_kst
        cutoff_start = now_kst - timedelta(hours=24) 

        # ì „ì²´ ê°€ì ¸ì˜¨ ê°œìˆ˜ ê¸°ë¡
        stats["total_fetched"] = len(feed.entries)
        # print(f"   ğŸ” [Debug] '{query}' ì›ë³¸ {stats['total_fetched']}ê°œ ë°œê²¬")

        for entry in feed.entries:
            # ëª©í‘œ ê°œìˆ˜ ì±„ìš°ë©´ ì¤‘ë‹¨
            if len(news_results) >= limit:
                break
                
            if len(seen_titles) >= fetch_count * 2:
                break

            # ---------------------------------------------------------
            # [í•„í„° 1] ìœ ë£Œ ë§¤ì²´(Paywall)
            # ---------------------------------------------------------
            source_name = entry.source.title if 'source' in entry else ""
            if is_paywalled(source_name):
                stats["dropped_paywall"] += 1
                # print(f"      ğŸš« [Skip:ìœ ë£Œ] {source_name}")
                continue

            # [í•„í„° 2] ë‚ ì§œ ì •ë°€ í•„í„°ë§
            pub_date_str = entry.published if 'published' in entry else ""
            try:
                dt_obj = date_parser.parse(pub_date_str)
                if dt_obj.tzinfo is None:
                    dt_obj = dt_obj.replace(tzinfo=pytz.utc)
                article_dt_kst = dt_obj.astimezone(kst_tz)
                
                if not (cutoff_start <= article_dt_kst <= cutoff_end):
                    stats["dropped_time"] += 1
                    # print(f"      â° [Skip:ì‹œê°„] {article_dt_kst.strftime('%m-%d %H:%M')} (ë²”ìœ„ ë°–)")
                    continue 
            except Exception:
                continue 

            # [í•„í„° 3] ì¤‘ë³µ ì œê±°
            title = entry.title
            is_dup = False
            for seen in seen_titles:
                if is_similar(title, seen):
                    is_dup = True
                    break
            
            if is_dup:
                stats["dropped_dup"] += 1
                # print(f"      ğŸ‘¯ [Skip:ì¤‘ë³µ] {title[:20]}...")
                continue 
            
            # -- í†µê³¼ --
            seen_titles.append(title)
            stats["accepted"] += 1
            
            pub_date_fmt = article_dt_kst.strftime("%Y-%m-%d %H:%M")
            
            news_results.append({
                "title": entry.title,
                "link": entry.link,
                "pub_date": pub_date_fmt,
                "source": source_name or "Google News"
            })
            
        # [ìµœì¢… ë¡œê·¸ ì¶œë ¥] ì™œ 0ê°œê°€ ë‚˜ì™”ëŠ”ì§€ í™•ì¸ ê°€ëŠ¥
        if stats["accepted"] == 0:
            print(f"   âš ï¸ [Result] '{query}' ìˆ˜ì§‘ 0ê±´! (ì›ì¸: ì‹œê°„íƒˆë½ {stats['dropped_time']}ê±´, ìœ ë£Œíƒˆë½ {stats['dropped_paywall']}ê±´, ì¤‘ë³µíƒˆë½ {stats['dropped_dup']}ê±´)")
            
        return news_results

    except Exception as e:
        print(f"RSS Error ({query}): {e}")
        return []

def analyze_news_sentiment(stock_name, news_list):
    """
    AIë¥¼ ì´ìš©í•œ íƒœê¹…, ì¤‘ìš”ë„ í‰ê°€, í‚¤ì›Œë“œ ë³¼ë“œ ì²˜ë¦¬
    """
    if not news_list:
        return []

    api_key = os.getenv("UPSTAGE_API_KEY")
    client = OpenAI(api_key=api_key, base_url="https://api.upstage.ai/v1/solar")

    news_context = ""
    for i, news in enumerate(news_list):
        news_context += f"[{i+1}] Source: {news['source']} | Title: {news['title']}\n"

    system_prompt = f"""
    You are a professional Stock News Analyst for '{stock_name}'.
    Analyze the provided news headlines.

    Tasks:
    1. **Sentiment**: Tag as 'ğŸŸ¢ í˜¸ì¬' (Good), 'ğŸ”´ ì•…ì¬' (Bad), or 'âšª ì¤‘ë¦½' (Neutral).
    2. **Importance**: Score from 1 (Trivial) to 5 (Critical Market Mover).
    3. **Keywords**: Identify 1-2 key words in the title and wrap them with markdown bold (**word**).
    4. **Translate**: If the title is in English, translate it to Korean naturally.

    Output format must be a JSON list of objects:
    [
        {{
            "sentiment": "ğŸŸ¢ í˜¸ì¬",
            "importance": 4,
            "processed_title": "Tesla **Earnings** beat expectations...",
            "korean_title": "í…ŒìŠ¬ë¼ **ì‹¤ì ** ì˜ˆìƒì¹˜ ìƒíšŒ..." 
        }}
    ]
    """

    try:
        response = client.chat.completions.create(
            model="solar-1-mini-chat",
            messages=[
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": news_context}
            ],
            temperature=0.1
        )
        
        content = response.choices[0].message.content
        cleaned = content.replace("```json", "").replace("```", "").strip()
        analysis_result = json.loads(cleaned)
        
        for i, item in enumerate(news_list):
            if i < len(analysis_result):
                ai_data = analysis_result[i]
                item["sentiment"] = ai_data.get("sentiment", "âšª ì¤‘ë¦½")
                item["importance"] = ai_data.get("importance", 1)
                
                if item.get("title") != ai_data.get("korean_title"):
                     item["display_title"] = ai_data.get("korean_title", item["title"])
                else:
                     item["display_title"] = ai_data.get("processed_title", item["title"])
            else:
                item["sentiment"] = "âšª ì¤‘ë¦½"
                item["display_title"] = item["title"]
                
        return news_list

    except Exception as e:
        print(f"AI Analysis Error: {e}")
        return news_list

def get_interested_stock_news():
    """
    ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
    """
    print("ğŸ“° ê´€ì‹¬ ì¢…ëª© ë‰´ìŠ¤ ìˆ˜ì§‘ ë° AI ë¶„ì„ ì‹œì‘...")
    results = []

    for stock in TARGET_STOCKS:
        ticker = stock["ticker"]
        name = stock["name"]
        lang = stock.get("lang", "en")
        limit = stock.get("limit", 2)

        print(f"   -> {name} ({ticker}) ë‰´ìŠ¤ ìˆ˜ì§‘ ì¤‘...")
        
        # 1. ë‰´ìŠ¤ ìˆ˜ì§‘
        raw_news = get_google_news_rss(name, lang, limit)
        
        # 2. AI ë¶„ì„
        if raw_news:
            analyzed_news = analyze_news_sentiment(name, raw_news)
            results.append({
                "ticker": ticker,
                "name": name,
                "news": analyzed_news
            })
        else:
             results.append({
                "ticker": ticker,
                "name": name,
                "news": [] 
            })
    
    return results